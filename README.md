# Backpropagation
The back-propagation algorithm is a non-linear extension of the least mean squares (LMS) algorithm for multi-layer perceptron. It is the most widely used of the neural network paradigms and has been successfully applied in many fields of model-free function estimation. The back propagation network (BPN) is expensive computationally, especially during the training process. Properly trained BPN tends to produce reasonable results when presented with new data set inputs. 
A BPN is usually layered, with each layer fully interconnected to the layers below and above it. The first layer is the input layer, the only layer in the network that can receive external input. The second layer is the hidden layer, in which the processing units are interconnected to the layers below and above it. The third layer is the output layer. Each unit of the hidden layer is interconnected with the units of the output layer. Units are not interconnected to other units within the same layer. Each interconnection is assigned an associative connection strength, expressed as weight (Figure 1). Weights are adjusted during the training of the network. In BPN, the training is supervised, in which case the network is presented with target values for each input pattern. The input space of the network is considered to be linearly separable. 
Behavior of hidden layer: 
1. We know that a non-linearly separable function like XOR cannot be computed by a single perceptron. So we need to introduce multi layered perceptron algorithm. There is 1 input layer containing 2 input neurons, 1 hidden layer containing 2 neurons and 1 output layer containing 1 neuron. We are iterating "times" number of times until the error we are getting is less than the desired error limit and the output we are getting are almost equal to the desired output. The no. of hidden layer can vary with needs. We are using Feed forward and Back propagation algorithm to train our perceptron. Initially zero is given to 
each weights and allowing the perceptron to train and adjust the weights in each iteration so as to create a definite decision boundary. Initially we have more errors but as the training goes on, the error value is decreases and slowly and steadily it converges means the error minimizes and perceptron tends to learn and it is more accurate with time. Here is an output..    
       
2. Hidden Layer in Perceptron  
 Hidden layer in a critical part of multi layered algorithm. It lies between input layer and output layer. The output value of hidden layer is not visible on output. 
 The usage of hidden layer in neural network is that it extends the abilities to learn logical operation and opens a wide area for complex computation.  
 We are working with one hidden layer. As we increase the nodes in hidden layer the algorithm tend to converge more quickly in less iterations. 
 As the number of hidden layer increase it can let the algorithm to learn more complex situation. But there is disadvantage with this, it also leads to higher cost and time.  
 So unless it’s a complex situation we need to minimize the use of more hidden layer.    
In the palindrome and odd parity problem, we are treating a number as 0 is it is less than 0.5 and 1 otherwise, based on this assumption we have trained the neural network to detect the palindrome pattern and gives the output close to 1 if true and close to 0 if false. 
The input is taken randomly and there can be any number of input neurons, hidden layer neurons and training sets. The expected and actual results matched after the convergence. 
